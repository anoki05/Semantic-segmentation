#U-net Architecture
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, BatchNormalization, Dropout, Lambda
from tensorflow.keras import backend as K


from tensorflow.keras.utils import normalize
import os
import glob
import cv2
import numpy as np
from matplotlib import pyplot as plt
import random 
from skimage.io import imshow
from PIL import Image
import pandas as pd
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split
import segmentation_models as sm
from tensorflow.keras.metrics import MeanIoU

def multiclass_unet_architecture(n_classes=2, height=256, width=256, channels=3):
    inputs = Input((height, width, channels))

    #Contraction path
    conv_1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(inputs)
    conv_1 = Dropout(0.1)(conv_1)  
    conv_1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(conv_1)
    pool_1 = MaxPooling2D((2, 2))(conv_1)
    
    conv_2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(pool_1)
    conv_2 = Dropout(0.1)(conv_2)  
    conv_2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(conv_2)
    pool_2 = MaxPooling2D((2, 2))(conv_2)
     
    conv_3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(pool_2)
    conv_3 = Dropout(0.1)(conv_3)
    conv_3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(conv_3)
    pool_3 = MaxPooling2D((2, 2))(conv_3)
     
    conv_4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(pool_3)
    conv_4 = Dropout(0.1)(conv_4)
    conv_4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(conv_4)
    pool_4 = MaxPooling2D(pool_size=(2, 2))(conv_4)
     
    conv_5 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(pool_4)
    conv_5 = Dropout(0.2)(conv_5)
    conv_5 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(conv_5)
    
    #Expansive path 
    u6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv_5)
    u6 = concatenate([u6, conv_4])
    conv_6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u6)
    conv_6 = Dropout(0.2)(conv_6)
    conv_6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(conv_6)
     
    u7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv_6)
    u7 = concatenate([u7, conv_3])
    conv_7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)
    conv_7 = Dropout(0.1)(conv_7)
    conv_7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(conv_7)
     
    u8 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv_7)
    u8 = concatenate([u8, conv_2])
    conv_8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8)
    conv_8 = Dropout(0.2)(conv_8)  
    conv_8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(conv_8)
     
    u9 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(conv_8)
    u9 = concatenate([u9, conv_1], axis=3)
    conv_9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9)
    conv_9 = Dropout(0.1)(conv_9)
    conv_9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(conv_9)
     
    outputs = Conv2D(n_classes, (1, 1), activation='softmax')(conv_9)
    
    model = Model(inputs=[inputs], outputs=[outputs])
    model.summary()
    return model


def jacard(y_true, y_pred):
    y_true_c = K.flatten(y_true)
    y_pred_c = K.flatten(y_pred)
    intersection = K.sum(y_true_c * y_pred_c)
    return (intersection + 1.0) / (K.sum(y_true_c) + K.sum(y_pred_c) - intersection + 1.0) 

def jacard_loss(y_true, y_pred):
    return -jacard(y_true,y_pred)

def importing_data(path):
    sample = []
    for filename in glob.glob(path):
        img = Image.open(filename,'r')
        img = img.resize((256,256))
        img = np.array(img)
        sample.append(img)  
    return sample

def importing_data(path):
    sample = []
    for filename in glob.glob(path):
        img = Image.open(filename,'r')
        img = img.resize((256,256))
        img = np.array(img)
        sample.append(img)  
    return sample

def image_labels(label):
    image_labels = np.zeros(label.shape, dtype=np.uint8)
    for i in range(24):
        image_labels [np.all(label == labels[i,:],axis=-1)] = i
    image_labels = image_labels[:,:,0]
    return image_labels

def get_model():
    return multiclass_unet_architecture(n_classes=n_classes, height=img_height, 
                           width=img_width, channels=img_channels)



train_path = r"DataSet\archive\semantic_drone_dataset\training_set\images/*.jpg"
data_train   = importing_data(train_path)
data_train = np.asarray(data_train)
mask_path = r"DataSet\archive\semantic_drone_dataset\training_set\gt\semantic\label_images/*.png"
data_mask   = importing_data(mask_path)
data_mask  = np.asarray(data_mask)

#%% Random visualization
x = random.randint(0, len(data_train))
plt.figure(figsize=(24,18))
plt.subplot(1,2,1)
imshow(data_train[x])
plt.subplot(1,2,2)
imshow(data_mask[x])
plt.show()

scaler = MinMaxScaler()
nsamples, nx, ny, nz = data_train.shape
d2_data_train = data_train.reshape((nsamples,nx*ny*nz))
train_images = scaler.fit_transform(d2_data_train)
train_images = train_images.reshape(400,256,256,3)

labels = pd.read_csv(r"DataSet\archive\semantic_drone_dataset\training_set\gt\semantic/class_dict.csv")
labels = labels.drop(['name'],axis = 1)
labels = np.array(labels)

label_final = []
for i in range(data_mask.shape[0]):
    label = image_labels(data_mask[i])
    label_final.append(label)    

label_final = np.array(label_final)  

"""5"""
n_classes = len(np.unique(label_final))
labels_cat = to_categorical(label_final, num_classes=n_classes)
x_train, x_test, y_train, y_test = train_test_split(train_images, labels_cat, test_size = 0.20, random_state = 42)

img_height = x_train.shape[1]
img_width  = x_train.shape[2]
img_channels = x_train.shape[3]


metrics=['accuracy', jacard]
model = get_model()
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=metrics)
model.summary()

history = model.fit(x_train, y_train, 
                    batch_size = 16, 
                    verbose=1, 
                    epochs=10, 
                    validation_data=(x_test, y_test), 
                    shuffle=False)
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(loss) + 1)
plt.plot(epochs, loss, 'y', label='Training loss')
plt.plot(epochs, val_loss, 'r', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

acc = history.history['jacard']
val_acc = history.history['val_jacard']

plt.plot(epochs, acc, 'y', label='Training Jaccard')
plt.plot(epochs, val_acc, 'r', label='Validation Jaccard')
plt.title('Training and validation Jacard')
plt.xlabel('Epochs')
plt.ylabel('Jaccard')
plt.legend()
plt.show()

y_pred=model.predict(x_test)
y_pred_argmax=np.argmax(y_pred, axis=3)
y_test_argmax=np.argmax(y_test, axis=3)

test_jacard = jacard(y_test,y_pred)
print(test_jacard)

fig, ax = plt.subplots(4, 3, figsize = (12,18)) 
for i in range(0,4):
    test_img_number = random.randint(0, len(x_test))
    test_img = x_test[test_img_number]
    ground_truth=y_test_argmax[test_img_number] 
    test_img_input=np.expand_dims(test_img, 0) 
    prediction = (model.predict(test_img_input)) 
    predicted_img=np.argmax(prediction, axis=3)[0,:,:]    
    
    ax[i,0].imshow(test_img)
    ax[i,0].set_title("RGB Image",fontsize=16)
    ax[i,1].imshow(ground_truth)
    ax[i,1].set_title("Ground Truth",fontsize=16)
    ax[i,2].imshow(predicted_img)
    ax[i,2].set_title("Prediction",fontsize=16)
    i+=i
    
plt.show()

BACKBONE = 'resnet34'
preprocess_input = sm.get_preprocessing(BACKBONE)
# preprocess input
x_train_new = preprocess_input(x_train)
x_test_new = preprocess_input(x_test)
model_resnet_backbone = sm.Unet(BACKBONE, encoder_weights=None, classes=n_classes, activation='softmax')
metrics=['accuracy', jacard]
model_resnet_backbone.compile(optimizer='adam', loss='categorical_crossentropy', metrics=metrics)
print(model_resnet_backbone.summary())

history_tf=model_resnet_backbone.fit(x_train_new, 
          y_train,
          batch_size=16, 
          epochs=5,
          verbose=1,
          validation_data=(x_test_new, y_test))
          
history = history_tf
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(loss) + 1)
plt.plot(epochs, loss, 'y', label='Training loss')
plt.plot(epochs, val_loss, 'r', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

acc = history.history['jacard']
val_acc = history.history['val_jacard']

plt.plot(epochs, acc, 'y', label='Training IoU')
plt.plot(epochs, val_acc, 'r', label='Validation IoU')
plt.title('Training and validation Jaccard')
plt.xlabel('Epochs')
plt.ylabel('Jaccard')
plt.legend()
plt.show()

y_pred_tf=model_resnet_backbone.predict(x_test)
y_pred_argmax_tf=np.argmax(y_pred_tf, axis=3)
y_test_argmax_tf=np.argmax(y_test, axis=3)

test_jacard = jacard(y_test,y_pred_tf)
print(test_jacard)

fig, ax = plt.subplots(5, 3, figsize = (12,18)) 
for i in range(0,5):

    test_img_number = random.randint(0, len(x_test))
    test_img_tf = x_test_new[test_img_number]
    ground_truth_tf=y_test_argmax_tf[test_img_number]
    test_img_input_tf=np.expand_dims(test_img_tf, 0)
    prediction_tf = (model_resnet_backbone.predict(test_img_input_tf))
    predicted_img_transfer_learning=np.argmax(prediction_tf, axis=3)[0,:,:]   
    
    ax[i,0].imshow(test_img_tf)
    ax[i,0].set_title("RGB Image",fontsize=16)
    ax[i,1].imshow(ground_truth_tf)
    ax[i,1].set_title("Ground Truth",fontsize=16)
    ax[i,2].imshow(predicted_img_transfer_learning)
    ax[i,2].set_title("Prediction(Transfer Learning)",fontsize=16)
    i+=i
    
plt.show()

fig, ax = plt.subplots(5, 4, figsize = (16,20)) 
for i in range(0,5):
    test_img_number = random.randint(0, len(x_test))
    
    test_img = x_test[test_img_number]
    ground_truth=y_test_argmax[test_img_number] 
    test_img_input=np.expand_dims(test_img, 0) 
    prediction = (model.predict(test_img_input)) 
    predicted_img=np.argmax(prediction, axis=3)[0,:,:]    
    
    test_img_tf = x_test_new[test_img_number]
    ground_truth_tf=y_test_argmax_tf[test_img_number]
    test_img_input_tf=np.expand_dims(test_img_tf, 0)
    prediction_tf = (model_resnet_backbone.predict(test_img_input_tf))
    predicted_img_transfer_learning=np.argmax(prediction_tf, axis=3)[0,:,:]  
    
    ax[i,0].imshow(test_img_tf)
    ax[i,0].set_title("RGB Image",fontsize=16)
    ax[i,1].imshow(ground_truth_tf)
    ax[i,1].set_title("Ground Truth",fontsize=16)
    ax[i,2].imshow(predicted_img)
    ax[i,2].set_title("Prediction",fontsize=16)
    ax[i,3].imshow(predicted_img_transfer_learning)
    ax[i,3].set_title("Prediction Transfer Learning",fontsize=16)
    
    i+=i   
plt.show()

